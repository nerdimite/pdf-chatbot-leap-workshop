{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "<h3>For Question Answering over PDF Documents</h3>\n",
    "\n",
    "This notebook demonstrates a vanilla implementation of a RAG pipeline for Question Answering (single-turn) over PDF documents. We start by ingesting the documents and indexing them using an open-source embedding model like `bge-small-en-v1.5` and a vector database like `chromadb`. We then move on to the two main components of the RAG pipeline: Retrieval and Generation. For retrieval, we use the same `bge-small-en-v1.5` model to retrieve the top-k relevant documents for a given query. For generation, we use an open-source LLM like `llama3-70b-8192` from Groq to generate the answer from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import Document, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"documents\", required_exts=[\".pdf\"], filename_as_id=True, file_metadata=None)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata(document: Document) -> Document:\n",
    "    document.metadata = {\n",
    "        \"page_label\": document.metadata[\"page_label\"],\n",
    "        \"file_name\": document.metadata[\"file_name\"]\n",
    "    }\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(map(clean_metadata, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='0233b043-dff3-41c4-960c-fb26022c5adc', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4744d2f8-b0ff-41dc-ada3-9f99111f27a4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dbc651dffb9d109abd81ea6a4de47b4281354da5c1bf3289da596491d5f2d125')}, text='Behavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.', start_char_idx=0, end_char_idx=2002, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='4744d2f8-b0ff-41dc-ada3-9f99111f27a4', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0233b043-dff3-41c4-960c-fb26022c5adc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='7f3f30aae12cc9e175355524bcbf078a75e18fdfc3afd81bc74a6574a21373a3')}, text='Methods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.\\nIn our approach, we use experts’ demonstrations to re-\\nshape the control problem as a search problem over a latent\\nspace of partial trajectories (called situations ). Our work as-\\nsumes that:\\n• Similar situations require similar solutions or actions.\\n• Asituation can be represented in a latent space.\\n* Equal contribution.\\nFigure 1: Similarity and divergence of ﬁve pairs of simu-\\nlator and dataset situation trajectories. Left column : cur-\\nrent frame from the MineRL environment. Middle col-\\numn : current reference frame from the dataset trajectory that\\nthe agent follows. Right column : L1-distance plot between\\nVPT embedding points of simulator and dataset situations .\\nThe current step shown as RGB images in the Left andMid-\\ndlecolumns is highlighted by the rightmost bold dot marker\\nin the Right column. X-axis indicates 256 time steps. Y-\\naxis indicates L1 distance between two embedding points\\nin the VPT latent space and ranges from 0.1 to 0.4. The de-\\nviation threshold is show by dashed red horizontal lines L1\\n= 0.35. Colored vertical lines mark new search events to ﬁnd\\nthe most similar situation in the dataset. Blue lines indicate\\ntime-based initiated searches, red lines indicate deviation-\\nthreshold based initiated searches. Gray dotted lines: every\\n64 steps in the X-axis and every 0.1 steps in the Y-axis.\\n• The situations latent space is a metric space. Therefore,\\nwe can assess the numerical similarity between any two\\nsituations .arXiv:2212.13326v2  [cs.LG]  17 Apr 2023', start_char_idx=1751, end_char_idx=3499, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    include_metadata=True,\n",
    ")\n",
    "splitter.get_nodes_from_documents([documents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an open source embedding model\n",
    "# bge_model_fn = SentenceTransformerEmbeddingFunction(model_name=\"BAAI/bge-small-en-v1.5\", device='cpu')\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Chroma VectorDB instance\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "chroma_collection = db.get_or_create_collection(\"rag\")\n",
    "\n",
    "# Create a Chroma VectorStore\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a Simple Document Store as well\n",
    "docstore = SimpleDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 86/86 [00:00<00:00, 226.42it/s]\n",
      "Generating embeddings: 100%|██████████| 307/307 [00:05<00:00, 58.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build the Embedding Pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        splitter,\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "_nodes = pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index for retrieval\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Explain behavioral cloning via search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='745d2310-9e9f-41a6-aebf-2474a92bb003', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d973f5a3-f7cf-4257-bff5-c853701c7e84', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dbc651dffb9d109abd81ea6a4de47b4281354da5c1bf3289da596491d5f2d125')}, text='Behavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.', start_char_idx=0, end_char_idx=2002, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6896332236196575),\n",
       " NodeWithScore(node=TextNode(id_='baa9226c-5cb8-4596-b17e-88cb106243f2', embedding=None, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='2f45bb3ba35b60c48f3d03c6fc7a95082b4828ac42488fd8e13efaae93122fc9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5ca2b937-38c0-459a-92ee-841dc89bc8f1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='e2be83dbaf7bf61b1d0d2d0c833d2d8fa93fabb18bcbe019af108faa84876897'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0a45ce61-dc58-4f76-8cf5-432bced89955', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c9eb3e919c5833773c77af78f15bfa93ae8f51f0e073fac91a6c81562cacb9f0')}, text='Additionally to the current frame, a memory stack\\nstores the last 128 embeddings for each transformer block.\\nThe output of the last transformer block are 129 embedding\\nvectors, each 1024-dimensional. The architecture discards\\n128 output embedding vectors of the last transformer block\\nand processes further only the current’s frame embedding\\nvector. Two MLP output heads take as an input the current’s\\nframe embedding vector to predict actions. The ﬁrst output\\nhead predicts a discrete action (one out of 8641 possible\\ncombinations of compound keyboard actions). The second\\noutput head predicts a computer mouse control as a discrete\\ncluster of one of the possible 121=11x11 mouse displace-\\nment regions (±5 regions for X times ±5 regions for Y). The\\narchitecture is shown in Figure 2.\\nSearch-based BC Search-based behavioral cloning (BC)\\naims to reproduce an expert’s behavior with high ﬁdelity by\\ncopying its solutions from past experience. We deﬁne a sit-\\nuation as a set {(oτ, aτ)}t+∆t\\nτ=tof consecutive observation-\\naction pairs coming from a set of provided expert’s trajecto-\\nries, where ∆tis less or equal to the number of input slots\\nof a transformer block that processes embedding vectors of\\ninput images.\\nWe encode the expert’s past situations through a provided\\nVPT model [6]. Thus, we obtain a latent space populated by\\nN-dimensional situation points. Due to the expert’s optimal-\\nity assumption, we can assume that each situation has been\\naddressed and solved in an optimal way.\\nVPT\\nDemonstration\\xa0\\nDatasetDataset Images\\nActionsVPT- Embeddings\\nSituations\\xa0\\nDataset\\nObserved Images VPT- EmbeddingsMost similar\\xa0\\nsituationL1 distance\\nMineRL\\xa0\\nEnvironmentSituations\\xa0\\nDataset\\nVPTActions\\nCop y\\xa0ActionsVPT- EmbeddingsA\\nBFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\\nation loop.\\nWe encode each sampled situation with the same net-\\nwork. Then, we search the nearest embedding point in the\\ndataset of situation points. Once the reference situation has\\nbeen selected, we copy its corresponding actions.', start_char_idx=1473, end_char_idx=3490, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6004311618081943)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Additionally to the current frame, a memory stack\n",
      "stores the last 128 embeddings for each transformer block.\n",
      "The output of the last transformer block are 129 embedding\n",
      "vectors, each 1024-dimensional. The architecture discards\n",
      "128 output embedding vectors of the last transformer block\n",
      "and processes further only the current’s frame embedding\n",
      "vector. Two MLP output heads take as an input the current’s\n",
      "frame embedding vector to predict actions. The ﬁrst output\n",
      "head predicts a discrete action (one out of 8641 possible\n",
      "combinations of compound keyboard actions). The second\n",
      "output head predicts a computer mouse control as a discrete\n",
      "cluster of one of the possible 121=11x11 mouse displace-\n",
      "ment regions (±5 regions for X times ±5 regions for Y). The\n",
      "architecture is shown in Figure 2.\n",
      "Search-based BC Search-based behavioral cloning (BC)\n",
      "aims to reproduce an expert’s behavior with high ﬁdelity by\n",
      "copying its solutions from past experience. We deﬁne a sit-\n",
      "uation as a set {(oτ, aτ)}t+∆t\n",
      "τ=tof consecutive observation-\n",
      "action pairs coming from a set of provided expert’s trajecto-\n",
      "ries, where ∆tis less or equal to the number of input slots\n",
      "of a transformer block that processes embedding vectors of\n",
      "input images.\n",
      "We encode the expert’s past situations through a provided\n",
      "VPT model [6]. Thus, we obtain a latent space populated by\n",
      "N-dimensional situation points. Due to the expert’s optimal-\n",
      "ity assumption, we can assume that each situation has been\n",
      "addressed and solved in an optimal way.\n",
      "VPT\n",
      "Demonstration \n",
      "DatasetDataset Images\n",
      "ActionsVPT- Embeddings\n",
      "Situations \n",
      "Dataset\n",
      "Observed Images VPT- EmbeddingsMost similar \n",
      "situationL1 distance\n",
      "MineRL \n",
      "EnvironmentSituations \n",
      "Dataset\n",
      "VPTActions\n",
      "Cop y ActionsVPT- EmbeddingsA\n",
      "BFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\n",
      "ation loop.\n",
      "We encode each sampled situation with the same net-\n",
      "work. Then, we search the nearest embedding point in the\n",
      "dataset of situation points. Once the reference situation has\n",
      "been selected, we copy its corresponding actions.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_nodes[1].node.get_content(metadata_mode='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_prompt(retrieved_nodes, query_str):\n",
    "    context_str = \"\\n--------------------\\n\".join([node.get_content(metadata_mode='all') for node in retrieved_nodes])\n",
    "    prompt = f\"Given the following context: \\n{context_str}\\n\\nPlease answer the following query: \\n{query_str}\\n\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the following context: \\npage_label: 1\\nfile_name: BC_via_Search_VPT.pdf\\n\\nBehavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.\\n--------------------\\npage_label: 2\\nfile_name: BC_via_Search_VPT.pdf\\n\\nAdditionally to the current frame, a memory stack\\nstores the last 128 embeddings for each transformer block.\\nThe output of the last transformer block are 129 embedding\\nvectors, each 1024-dimensional. The architecture discards\\n128 output embedding vectors of the last transformer block\\nand processes further only the current’s frame embedding\\nvector. Two MLP output heads take as an input the current’s\\nframe embedding vector to predict actions. The ﬁrst output\\nhead predicts a discrete action (one out of 8641 possible\\ncombinations of compound keyboard actions). The second\\noutput head predicts a computer mouse control as a discrete\\ncluster of one of the possible 121=11x11 mouse displace-\\nment regions (±5 regions for X times ±5 regions for Y). The\\narchitecture is shown in Figure 2.\\nSearch-based BC Search-based behavioral cloning (BC)\\naims to reproduce an expert’s behavior with high ﬁdelity by\\ncopying its solutions from past experience. We deﬁne a sit-\\nuation as a set {(oτ, aτ)}t+∆t\\nτ=tof consecutive observation-\\naction pairs coming from a set of provided expert’s trajecto-\\nries, where ∆tis less or equal to the number of input slots\\nof a transformer block that processes embedding vectors of\\ninput images.\\nWe encode the expert’s past situations through a provided\\nVPT model [6]. Thus, we obtain a latent space populated by\\nN-dimensional situation points. Due to the expert’s optimal-\\nity assumption, we can assume that each situation has been\\naddressed and solved in an optimal way.\\nVPT\\nDemonstration\\xa0\\nDatasetDataset Images\\nActionsVPT- Embeddings\\nSituations\\xa0\\nDataset\\nObserved Images VPT- EmbeddingsMost similar\\xa0\\nsituationL1 distance\\nMineRL\\xa0\\nEnvironmentSituations\\xa0\\nDataset\\nVPTActions\\nCop y\\xa0ActionsVPT- EmbeddingsA\\nBFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\\nation loop.\\nWe encode each sampled situation with the same net-\\nwork. Then, we search the nearest embedding point in the\\ndataset of situation points. Once the reference situation has\\nbeen selected, we copy its corresponding actions.\\n\\nPlease answer the following query: \\nExplain behavioral cloning via search\\n\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_context_prompt(retrieved_nodes, query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(build_context_prompt(retrieved_nodes, query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavioral cloning via search is a method for reproducing an expert's behavior with high fidelity by copying its solutions from past experience. It involves encoding the expert's past situations through a provided model, such as a Video PreTraining (VPT) model, to obtain a latent space populated by N-dimensional situation points. Each situation is assumed to have been addressed and solved in an optimal way due to the expert's optimality assumption.\n",
      "\n",
      "In this method, each sampled situation is encoded with the same network, and the nearest embedding point in the dataset of situation points is searched. Once the reference situation has been selected, the corresponding actions are copied. This process is repeated in an evaluation loop, where the agent copies the actions from the expert trajectory as long as the distance between the state representations of the agent and the selected expert trajectory from the dataset do not diverge. The proximity search is then repeated to find new expert trajectories for the agent to follow. This approach can effectively recover meaningful demonstration trajectories and show human-like behavior of an agent in the Minecraft environment.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot \n",
    "<h3>For Conversational Question Answering over PDF Documents</h3>\n",
    "\n",
    "Now that we have a basic RAG pipeline in place, we can extend it to a conversational setting. We can use the same retrieval and generation components as before, but we also need to keep track of the conversation but more importantly, we need to intelligently call the RAG pipeline based on the context. We will create an Agent with function calling capabilities to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.agent.openai import OpenAIAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Agent Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "\n",
    "\n",
    "def useless_tool() -> int:\n",
    "    \"\"\"This is a uselss tool.\"\"\"\n",
    "    return \"This is a uselss output.\"\n",
    "\n",
    "\n",
    "useless_tool = FunctionTool.from_defaults(fn=useless_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the OpenAIAgent class to use function calling capabilities with our Groq LLM\n",
    "agent = OpenAIAgent.from_tools([useless_tool, add_tool], llm=llm, verbose=True, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Hey there\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":5,\"b\":3}\n",
      "Got output: 8\n",
      "========================\n",
      "\n",
      "Great, the tool \"add\" was called with parameters a=5 and b=3, and it returned the sum of 8. So, 5 plus 3 equals 8.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Hey there\", tool_choice=\"auto\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_fn(query: str) -> str:\n",
    "    \"\"\"This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string containing the retrieved context chunks from the document.\n",
    "    \"\"\"\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    context_str = \"\\n--------------------\\n\".join([node.get_content(metadata_mode='all') for node in retrieved_nodes])\n",
    "    return \"Contextual information to answer the query is below:\\n\" + context_str\n",
    "\n",
    "retrieval_tool = FunctionTool.from_defaults(fn=retrieval_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = ChatMessage(\n",
    "    role='system',\n",
    "    content=\"\"\"You are a Q&A bot. You are here to answer questions based on the context given.\n",
    "You are prohibited from using prior knowledge and you can only use the context given. If you need \n",
    "more information, please ask the user.\n",
    "\n",
    "You have access to the `retrieval_fn` function which can be used to retrieve relevant context chunks from the document based on a query.\n",
    "You don't need to call this function always, but you can use it when you need to retrieve context from the document based on the query.\n",
    "Use only when absolutely necessary.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = OpenAIAgent.from_tools(\n",
    "    tools=[retrieval_tool],\n",
    "    llm=llm,\n",
    "    chat_history=[system_message],\n",
    "    max_function_calls=1,\n",
    "    temperature=0.2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are a Q&A bot. You are here to answer questions based on the context given.\\nYou are prohibited from using prior knowledge and you can only use the context given. If you need \\nmore information, please ask the user.\\n\\nYou have access to the `retrieval_fn` function which can be used to retrieve relevant context chunks from the document based on a query.\\nYou don't need to call this function always, but you can use it when you need to retrieve context from the document based on the query.\\nUse only when absolutely necessary.\\n\", additional_kwargs={})]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain behavioral cloning via search\n",
      "=== Calling Function ===\n",
      "Calling function: retrieval_fn with args: {\"query\":\"behavioral cloning via search\"}\n",
      "Got output: Contextual information to answer the query is below:\n",
      "page_label: 1\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Behavioral Cloning via Search in Video PreTraining Latent Space\n",
      "Federico Malato*\n",
      "University of Eastern Finland\n",
      "fmalato@uef.ﬁFlorian Leopold*\n",
      "University of Bielefeld\n",
      "ﬂeopold@techfak.uni-bielefeld.deAmogh Raut\n",
      "Indian Institute of Technology BHU\n",
      "Ville Hautam ¨aki\n",
      "University of Eastern FinlandAndrew Melnik\n",
      "University of Bielefeld\n",
      "Abstract\n",
      "Our aim is to build autonomous agents that can solve\n",
      "tasks in environments like Minecraft. To do so, we used\n",
      "an imitation learning-based approach. We formulate our\n",
      "control problem as a search problem over a dataset of\n",
      "experts’ demonstrations, where the agent copies actions\n",
      "from a similar demonstration trajectory of image-action\n",
      "pairs. We perform a proximity search over the BASALT\n",
      "MineRL-dataset in the latent representation of a Video\n",
      "PreTraining model. The agent copies the actions from\n",
      "the expert trajectory as long as the distance between the\n",
      "state representations of the agent and the selected ex-\n",
      "pert trajectory from the dataset do not diverge. Then\n",
      "the proximity search is repeated. Our approach can\n",
      "effectively recover meaningful demonstration trajecto-\n",
      "ries and show human-like behavior of an agent in the\n",
      "Minecraft environment.\n",
      "Introduction\n",
      "This study was motivated by the MineRL BASALT 2022\n",
      "challenge [1]. In the challenge, an agent must solve the fol-\n",
      "lowing tasks: ﬁnd a cave, catch a pet, build a village house,\n",
      "and make a waterfall [1]. The provided dataset of experts’\n",
      "demonstrations contains trajectories of image-action pairs.\n",
      "Additionally, both the MineRL BASALT dataset and envi-\n",
      "ronments do not contain reward information. Therefore, our\n",
      "primary focus was on Behavioural Cloning (BC) and Plan-\n",
      "ning [2][3] methods to address the tasks, rather than deep\n",
      "reinforcement learning (DRL) [4][5].\n",
      "Methods\n",
      "A dataset of expert demonstrations solving the following\n",
      "tasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\n",
      "lage house, and make a waterfall. Each episode is a trajec-\n",
      "tory of image-action pairs. No reward information is pro-\n",
      "vided.\n",
      "--------------------\n",
      "page_label: 2\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Additionally to the current frame, a memory stack\n",
      "stores the last 128 embeddings for each transformer block.\n",
      "The output of the last transformer block are 129 embedding\n",
      "vectors, each 1024-dimensional. The architecture discards\n",
      "128 output embedding vectors of the last transformer block\n",
      "and processes further only the current’s frame embedding\n",
      "vector. Two MLP output heads take as an input the current’s\n",
      "frame embedding vector to predict actions. The ﬁrst output\n",
      "head predicts a discrete action (one out of 8641 possible\n",
      "combinations of compound keyboard actions). The second\n",
      "output head predicts a computer mouse control as a discrete\n",
      "cluster of one of the possible 121=11x11 mouse displace-\n",
      "ment regions (±5 regions for X times ±5 regions for Y). The\n",
      "architecture is shown in Figure 2.\n",
      "Search-based BC Search-based behavioral cloning (BC)\n",
      "aims to reproduce an expert’s behavior with high ﬁdelity by\n",
      "copying its solutions from past experience. We deﬁne a sit-\n",
      "uation as a set {(oτ, aτ)}t+∆t\n",
      "τ=tof consecutive observation-\n",
      "action pairs coming from a set of provided expert’s trajecto-\n",
      "ries, where ∆tis less or equal to the number of input slots\n",
      "of a transformer block that processes embedding vectors of\n",
      "input images.\n",
      "We encode the expert’s past situations through a provided\n",
      "VPT model [6]. Thus, we obtain a latent space populated by\n",
      "N-dimensional situation points. Due to the expert’s optimal-\n",
      "ity assumption, we can assume that each situation has been\n",
      "addressed and solved in an optimal way.\n",
      "VPT\n",
      "Demonstration \n",
      "DatasetDataset Images\n",
      "ActionsVPT- Embeddings\n",
      "Situations \n",
      "Dataset\n",
      "Observed Images VPT- EmbeddingsMost similar \n",
      "situationL1 distance\n",
      "MineRL \n",
      "EnvironmentSituations \n",
      "Dataset\n",
      "VPTActions\n",
      "Cop y ActionsVPT- EmbeddingsA\n",
      "BFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\n",
      "ation loop.\n",
      "We encode each sampled situation with the same net-\n",
      "work. Then, we search the nearest embedding point in the\n",
      "dataset of situation points. Once the reference situation has\n",
      "been selected, we copy its corresponding actions.\n",
      "========================\n",
      "\n",
      "Behavioral cloning via search involves using a search algorithm to find the most similar situation in a dataset of expert demonstrations, based on the current situation. The current situation is encoded through a provided Video PreTraining (VPT) model, which produces a latent space populated by N-dimensional situation points. Each situation in the dataset is assumed to have been addressed and solved in an optimal way, due to the expert's optimality assumption. Once the most similar situation has been selected, the corresponding actions are copied and used by the agent. This process is repeated until the agent is able to effectively recover meaningful demonstration trajectories and show human-like behavior in the target environment. In the case of the Minecraft environment, the agent is trained to solve tasks such as finding a cave, catching a pet, building a village house, and making a waterfall, based on a dataset of expert demonstrations in the form of image-action pairs. The search for the most similar situation is performed in the latent representation of a Video PreTraining model, and the agent copies the actions from the expert trajectory as long as the distance between the state representations of the agent and the selected expert trajectory from the dataset does not diverge. The proximity search is then repeated. This approach has been shown to be effective in recovering meaningful demonstration trajectories and showing human-like behavior in the Minecraft environment.\n"
     ]
    }
   ],
   "source": [
    "response = rag_agent.chat(\n",
    "    \"Explain behavioral cloning via search\", tool_choice=\"auto\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: ELI5 your answer\n",
      "Behavioral cloning via search is a way for a computer program to learn how to do something by looking at examples of how an expert did it. It's like if you were trying to learn how to play a video game, and you watched a video of someone else playing the game and copied what they did. In this case, the computer program looks at a bunch of examples of an expert playing a game called Minecraft, and it tries to find examples that are similar to what it is currently seeing. When it finds a similar example, it copies what the expert did in that example. It keeps doing this over and over again, trying to find the best example to follow each time, until it can play the game like a human would.\n"
     ]
    }
   ],
   "source": [
    "response = rag_agent.chat(\n",
    "    \"ELI5 your answer\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

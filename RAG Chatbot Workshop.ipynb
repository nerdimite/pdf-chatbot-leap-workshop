{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "<h3>For Question Answering over PDF Documents</h3>\n",
    "\n",
    "This notebook demonstrates a vanilla implementation of a RAG pipeline for Question Answering (single-turn) over PDF documents. We start by ingesting the documents and indexing them using an open-source embedding model like `bge-small-en-v1.5` and a vector database like `chromadb`. We then move on to the two main components of the RAG pipeline: Retrieval and Generation. For retrieval, we use the same `bge-small-en-v1.5` model to retrieve the top-k relevant documents for a given query. For generation, we use an open-source LLM like `llama3-70b-8192` from Groq to generate the answer from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavesh/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import Document, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"documents\", required_exts=[\".pdf\"], filename_as_id=True, file_metadata=None)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata(document: Document) -> Document:\n",
    "    document.metadata = {\n",
    "        \"page_label\": document.metadata[\"page_label\"],\n",
    "        \"file_name\": document.metadata[\"file_name\"]\n",
    "    }\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(map(clean_metadata, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    include_metadata=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='519eed3b-15e7-4fd1-b5b2-a975734eba0d', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='96c673df-d138-4d27-8b79-c9ba35519145', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dbc651dffb9d109abd81ea6a4de47b4281354da5c1bf3289da596491d5f2d125')}, text='Behavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.', start_char_idx=0, end_char_idx=2002, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='96c673df-d138-4d27-8b79-c9ba35519145', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='519eed3b-15e7-4fd1-b5b2-a975734eba0d', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='7f3f30aae12cc9e175355524bcbf078a75e18fdfc3afd81bc74a6574a21373a3')}, text='Methods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.\\nIn our approach, we use experts’ demonstrations to re-\\nshape the control problem as a search problem over a latent\\nspace of partial trajectories (called situations ). Our work as-\\nsumes that:\\n• Similar situations require similar solutions or actions.\\n• Asituation can be represented in a latent space.\\n* Equal contribution.\\nFigure 1: Similarity and divergence of ﬁve pairs of simu-\\nlator and dataset situation trajectories. Left column : cur-\\nrent frame from the MineRL environment. Middle col-\\numn : current reference frame from the dataset trajectory that\\nthe agent follows. Right column : L1-distance plot between\\nVPT embedding points of simulator and dataset situations .\\nThe current step shown as RGB images in the Left andMid-\\ndlecolumns is highlighted by the rightmost bold dot marker\\nin the Right column. X-axis indicates 256 time steps. Y-\\naxis indicates L1 distance between two embedding points\\nin the VPT latent space and ranges from 0.1 to 0.4. The de-\\nviation threshold is show by dashed red horizontal lines L1\\n= 0.35. Colored vertical lines mark new search events to ﬁnd\\nthe most similar situation in the dataset. Blue lines indicate\\ntime-based initiated searches, red lines indicate deviation-\\nthreshold based initiated searches. Gray dotted lines: every\\n64 steps in the X-axis and every 0.1 steps in the Y-axis.\\n• The situations latent space is a metric space. Therefore,\\nwe can assess the numerical similarity between any two\\nsituations .arXiv:2212.13326v2  [cs.LG]  17 Apr 2023', start_char_idx=1751, end_char_idx=3499, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex. Splitting the first document\n",
    "splitter.get_nodes_from_documents([documents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an open source embedding model\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='126964cc-7055-4c6e-b79b-d9bda8825927', embedding=[-0.05667181313037872, 0.05341920629143715, -0.021931292489171028, 0.004545822739601135, -0.027262045070528984, 0.028728431090712547, 0.01862359791994095, -0.0016266194870695472, 0.010081356391310692, 0.026402493938803673, 0.0513732023537159, -0.07471895962953568, -0.021772367879748344, 0.053887516260147095, 0.030736912041902542, 0.045956775546073914, -0.049757830798625946, 0.05065251514315605, 0.047887712717056274, -0.01884228177368641, -0.007115844637155533, -0.049016956239938736, 0.03415176272392273, -0.052417825907468796, -0.021597567945718765, 0.006594723090529442, -0.04067232459783554, -0.049976807087659836, 0.021770469844341278, -0.21818138659000397, 0.04820670560002327, -0.011911135166883469, 0.038087014108896255, -0.007789707742631435, 0.0052032507956027985, 0.019187867641448975, -0.038507286459207535, 0.03414782136678696, -0.06089456006884575, 0.027183813974261284, 0.02287820540368557, 0.031756315380334854, 0.016321664676070213, -0.013401651754975319, -0.011027306318283081, -0.02726278454065323, 0.022700082510709763, -0.07763984054327011, -0.03211112320423126, -0.0200765710324049, 0.013193291611969471, -0.08287346363067627, -0.030196847394108772, 0.01776149682700634, 0.05517291650176048, -0.016912365332245827, 0.04512477666139603, 0.08078249543905258, 0.02726970799267292, -0.0013945101527497172, 0.058770667761564255, 0.018607500940561295, -0.08317006379365921, 0.039570149034261703, 0.04955167695879936, 0.0618431493639946, -0.013064123690128326, -0.044041432440280914, 0.010334818623960018, 0.010568288154900074, -0.013031812384724617, 0.07354827970266342, 0.027022117748856544, 0.01340936217457056, 0.045180466026067734, 0.025510454550385475, -0.02931027114391327, -0.017067119479179382, 0.016947921365499496, -0.06869121640920639, -0.0497313067317009, -0.005879376549273729, -0.006492388900369406, -0.006057912018150091, -0.052950333803892136, 0.0015297195641323924, 0.039872899651527405, -0.03055202029645443, 0.011231305077672005, 0.008109841495752335, -0.01610589399933815, 0.021813007071614265, 0.004583894275128841, -0.00024503233726136386, -0.06591447442770004, 0.012945263646543026, 0.0005144308088347316, 0.03503299504518509, -0.019636690616607666, 0.4102174937725067, 0.016137072816491127, 0.04670707508921623, 0.0215125884860754, 0.03050122782588005, 0.010176236741244793, -0.0255203228443861, -0.07206087559461594, -0.00753085408359766, 0.0006426491891033947, 0.02581099420785904, -0.05406922474503517, 0.0077920095063745975, 0.0610099695622921, -0.017918892204761505, -0.010220431722700596, 0.0005220591556280851, 0.014182507060468197, 0.04278689622879028, 0.05051083490252495, -0.02403944730758667, -0.011192738078534603, -0.03182446211576462, 0.07649265229701996, -0.06754197180271149, -0.010101310908794403, -0.028898026794195175, -0.009549261070787907, 0.10315736383199692, 0.022514868527650833, -0.010533405467867851, 0.02831662818789482, 0.013573256321251392, -0.04697727784514427, 0.024147648364305496, 0.00229079881682992, 0.039921797811985016, -0.000829583324957639, -0.06305063515901566, -0.05291255563497543, 0.005512986332178116, 0.027217889204621315, 0.07518663257360458, 0.08462207764387131, -0.028186753392219543, -0.031595345586538315, 0.0488499216735363, 0.04017529636621475, -0.029967382550239563, 0.009212643839418888, -0.04178217053413391, -0.048112571239471436, 0.05144300311803818, -0.044012174010276794, 0.016335470601916313, -0.03553452342748642, 0.022444387897849083, 0.04435314983129501, 0.005687506403774023, -0.09515783935785294, -0.008264121599495411, -0.03595131263136864, 0.0002948327164631337, 0.029033873230218887, 0.09821684658527374, 0.023749586194753647, -0.07086590677499771, -0.04755282402038574, 0.012249569408595562, -0.005787615664303303, -0.010996161960065365, -0.014318187721073627, 0.03383535146713257, -0.03792702779173851, -0.008754431270062923, -0.021081306040287018, -0.00852997787296772, -0.12085839360952377, 0.015059363096952438, -0.022883763536810875, 0.04177771136164665, -0.00039309938438236713, -0.08946764469146729, 0.04782538488507271, 0.0100364675745368, -0.008445625193417072, -0.02037537470459938, -0.008043020032346249, 0.010220426134765148, 0.032667629420757294, 0.03440983593463898, 0.00032023442327044904, -0.0024302839301526546, -0.04285268485546112, 0.030413703992962837, -0.0046823169104754925, -0.02317901886999607, 0.018488505855202675, -0.017953163012862206, -0.044831935316324234, -0.009088978171348572, -0.016453580930829048, 0.01789850927889347, -0.030532343313097954, -0.026794688776135445, -0.04211777076125145, -0.012202944606542587, 0.01840205118060112, -0.05743159353733063, 0.06328541785478592, -0.00045835800119675696, -0.04522963613271713, -0.02793125808238983, -0.06616147607564926, 0.016517555341124535, 0.0017136269016191363, 0.01933961734175682, 0.03433527424931526, 0.002242976799607277, -0.0020296908915042877, 0.041680630296468735, -0.03840050473809242, -0.02916325069963932, 0.01876012049615383, -0.2985606789588928, -0.07902707159519196, -0.013765170238912106, 0.025245817378163338, 0.0260290689766407, -0.04070110246539116, 0.02224528230726719, -0.016295764595270157, 0.04153267294168472, -0.023155955597758293, 0.06876903027296066, -0.036811038851737976, -0.009882694110274315, 0.021056033670902252, 0.0003789912152569741, 0.005043971352279186, 0.04030092805624008, 0.04772971570491791, -0.026251859962940216, 0.030209187418222427, 0.021765172481536865, -0.057697366923093796, 0.033114876598119736, -0.09857948869466782, 0.018673568964004517, 0.0005614763940684497, 0.14720259606838226, -5.234410855337046e-05, 0.03530263528227806, 0.006733176298439503, -0.03531857952475548, 0.04792667180299759, -0.07390279322862625, -0.06242174282670021, 0.015342500060796738, -0.016281912103295326, 0.08495364338159561, 0.00630943151190877, 0.0010264156153425574, -0.05315028876066208, -0.09017298370599747, -0.0023857280611991882, 0.013620661571621895, -0.08975536376237869, -0.043900132179260254, 0.01612027734518051, -0.062122851610183716, 0.049638573080301285, -0.01640843041241169, -0.005955039989203215, 0.060710981488227844, 0.015063549391925335, 0.0311770960688591, -0.07430706173181534, -0.053997062146663666, -0.025955041870474815, -0.04304793104529381, 0.0030915706884115934, -0.03269073739647865, 0.03073672577738762, 0.00944572128355503, -0.03346000611782074, -0.013846603222191334, -0.037754908204078674, 0.07052095979452133, -0.02107437327504158, 0.0299825556576252, -0.02789944037795067, 0.04077858105301857, 0.04050217568874359, -0.03688252344727516, 0.09534496068954468, 0.00877933669835329, 0.017611226066946983, 0.019335836172103882, -0.008985771797597408, -0.015480507165193558, -0.006593198515474796, -0.0038187867030501366, 0.04298236966133118, 0.012356487102806568, -0.0037254097405821085, 0.038284074515104294, 0.014713725075125694, 0.04375400021672249, -0.004455269314348698, 0.008678627200424671, -0.027848172932863235, 0.04733521118760109, 0.021826010197401047, 0.025424547493457794, 0.008805026300251484, -0.05697835981845856, -0.04297693818807602, 0.018124280497431755, 0.04093732312321663, -0.26604509353637695, -0.0052795023657381535, 0.04152775928378105, 0.04883989691734314, -0.026971828192472458, -0.026647090911865234, 0.053857315331697464, -0.04166841134428978, 0.0452674999833107, -0.0004810127429664135, 0.0011667244834825397, 0.0376509353518486, 0.06432966887950897, 0.061469197273254395, -0.020148735493421555, 0.03578894957900047, 0.08421996235847473, -0.01021803729236126, 0.021137140691280365, -0.056001581251621246, -0.01609382964670658, 0.01672430895268917, 0.20951052010059357, -0.032576777040958405, 0.04451413452625275, 0.0033944898750633, 0.0044016470201313496, -0.08213277906179428, 0.017654480412602425, -0.02321721240878105, -0.016413692384958267, -0.02393074333667755, 0.07249299436807632, -0.004753082990646362, 0.02651835046708584, 0.03166540339589119, -0.01345792505890131, 0.015449008904397488, -0.016162414103746414, -0.01756589487195015, -0.00713802594691515, 0.010218026116490364, 0.06666182726621628, -0.027870599180459976, 0.06984918564558029, 0.015735741704702377, 0.022659052163362503, -0.020744798704981804, -0.05906737223267555, 0.009006962180137634, -0.04651681333780289, -0.009370412677526474, -0.006678060162812471, -0.043516285717487335, -0.005125710740685463, 0.018490105867385864, -0.0027719836216419935, 0.0023473298642784357, -0.018324730917811394, -0.03470553085207939, 0.011718129739165306, -0.011850087903439999, 0.0986466258764267, -0.024241898208856583, 0.006015530787408352], metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e7c0cc03-3401-4740-95af-41867afad0a6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dbc651dffb9d109abd81ea6a4de47b4281354da5c1bf3289da596491d5f2d125')}, text='Behavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.', start_char_idx=0, end_char_idx=2002, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='e7c0cc03-3401-4740-95af-41867afad0a6', embedding=[-0.027048008516430855, -0.004938177298754454, -0.030717862769961357, 0.014730827882885933, 0.003083981806412339, 0.062466394156217575, 0.006433920003473759, 0.014234231784939766, 0.04534553736448288, 0.040862731635570526, 0.016898255795240402, -0.031306348741054535, 0.006454487331211567, 0.05508847534656525, 0.01383638009428978, 0.025334255769848824, -0.04903244227170944, 0.030454112216830254, 0.01866311766207218, -0.010558268055319786, 0.0018190793925896287, -0.009838484227657318, 0.04790524020791054, -0.04782518744468689, -0.02009081281721592, 0.03730127587914467, -0.01602131314575672, -0.02924872562289238, -0.0022026996593922377, -0.22144918143749237, 0.025997178629040718, -0.018887288868427277, 0.05413937568664551, -0.027666904032230377, -0.007641940843313932, 0.037584129720926285, -0.01912851259112358, 0.03713569417595863, -0.04999566078186035, -0.039211373776197433, 0.006625368259847164, 0.009998342022299767, 0.0036045394372195005, -0.037862468510866165, -0.011331786401569843, -0.0315050408244133, 0.015014994889497757, -0.04911943897604942, -0.04553062096238136, -0.024993987753987312, 0.015212052501738071, -0.07279449701309204, -0.03822765126824379, 0.010671643540263176, 0.07149774581193924, -0.017130188643932343, 0.06730049848556519, 0.03708236292004585, 0.004534367937594652, 0.018948350101709366, 0.0728750005364418, 0.007671723607927561, -0.10694487392902374, 0.029989782720804214, 0.03479807451367378, 0.01677095890045166, 0.004109666217118502, -0.012199318036437035, 0.04002412036061287, 0.026137730106711388, -0.009992045350372791, 0.01888909377157688, 0.009809235110878944, 0.05575673282146454, 0.038531411439180374, 0.027119377627968788, -0.022327188402414322, 0.012431113980710506, -0.0319516696035862, -0.07949467748403549, -0.004179187119007111, 0.0423288531601429, -0.001960043329745531, -0.010538888163864613, -0.03547077625989914, -0.019280415028333664, 0.017060689628124237, -0.016480445861816406, 0.030692987143993378, -0.007534595672041178, -0.035806089639663696, 0.023571543395519257, -0.013716768473386765, 0.005096471402794123, -0.05002748593688011, -0.048128850758075714, 0.024305876344442368, -0.0028889221139252186, 0.028957292437553406, 0.4175277352333069, 0.006171759217977524, 0.04720206931233406, 0.027659928426146507, 0.04683396592736244, 0.02091447450220585, -0.048313844949007034, -0.021165024489164352, -0.010308882221579552, 0.0021169784013181925, 0.005498845595866442, -0.05960048362612724, -0.02529536560177803, 0.0679570659995079, -0.06080810725688934, 5.847971624461934e-05, -0.01004431676119566, -0.022992299869656563, 0.05050857365131378, 0.04245416447520256, -0.03648443892598152, -0.011586970649659634, 0.003439160995185375, 0.0344766303896904, -0.044286902993917465, -0.007022933568805456, -0.03790942206978798, 0.012887063436210155, 0.10589002817869186, 0.016205890104174614, -0.02509978599846363, 0.029452281072735786, 0.014984765090048313, -0.03667641803622246, 0.03270787373185158, -0.009221119806170464, 0.04446128383278847, 0.020281348377466202, 0.021429942920804024, -0.04048028215765953, 0.007643275428563356, -0.009477034211158752, 0.046764738857746124, 0.08464892208576202, -0.029708359390497208, -0.05826738849282265, 0.14683569967746735, -0.01560584083199501, -0.04037352651357651, 0.013280147686600685, -0.024022381752729416, -0.016135532408952713, 0.04976661875844002, -0.0003664454852696508, 0.0009579127072356641, -0.0389106385409832, 0.04104124382138252, 0.05102907493710518, -0.029476137831807137, -0.06982725858688354, -0.006102390121668577, -0.022409461438655853, -0.045548100024461746, 0.00051089160842821, 0.09098009765148163, 0.013536040671169758, -0.09151863306760788, -0.03554416075348854, -0.01452517881989479, 0.015252884477376938, -0.0009350173640996218, -0.005201986525207758, 0.006045826245099306, -0.014497662894427776, 0.021049143746495247, 0.005644450429826975, 0.0026764485519379377, -0.10657358914613724, 0.010490979067981243, 0.007593580987304449, 0.03992382064461708, 0.0010969550348818302, -0.08299390971660614, 0.04000984504818916, 0.04136409983038902, -0.0011014161864295602, -0.0037225845735520124, 0.007512904237955809, 0.03370719775557518, 0.031458474695682526, 0.06598363071680069, -0.003781961277127266, -0.05414368584752083, -0.021689843386411667, 0.06109011545777321, 0.009649310261011124, -0.018104147166013718, 0.01806199736893177, -0.033851612359285355, -0.04390077292919159, -0.016655830666422844, -0.006668774411082268, -0.022061297670006752, 0.006567974109202623, 0.0019415970891714096, -0.04473647475242615, -0.02899085357785225, -0.010336088947951794, -0.05592764541506767, 0.009458660148084164, 0.03217553347349167, -0.08065660297870636, -0.026335980743169785, -0.06411341577768326, -0.0224180705845356, 0.03911583498120308, 0.009495057165622711, 0.018673919141292572, 0.024836575612425804, -0.01586242951452732, 0.05416172370314598, -0.02113916724920273, -0.01587788760662079, 0.030764751136302948, -0.35954907536506653, -0.060531727969646454, -0.01749173179268837, 0.016550028696656227, 0.046444036066532135, -0.022703438997268677, -0.007950497791171074, -0.029922664165496826, 0.00745454803109169, 0.010068229399621487, 0.044310394674539566, -0.03342282772064209, -0.00770169822499156, -0.0467117503285408, 0.015153425745666027, -0.00396773824468255, 0.03345359489321709, 0.06727316975593567, -0.02163996361196041, -0.020190931856632233, -0.004755131900310516, -0.029941048473119736, 0.023103022947907448, -0.07081509381532669, 0.03346402943134308, -0.010693812742829323, 0.1479790061712265, 0.0010405267821624875, 0.03393833339214325, -0.019095929339528084, -0.007939689792692661, 0.03461237996816635, -0.04538210108876228, -0.026309244334697723, 0.029930679127573967, -0.00020031657186336815, 0.09443089365959167, 0.009469329379498959, 0.011768803931772709, -0.03603161498904228, -0.059658680111169815, -0.028723668307065964, 0.012489770539104939, -0.030412500724196434, -0.038262564688920975, -0.011083505116403103, -0.04378494247794151, 0.029581066220998764, -0.023012425750494003, 0.009179214015603065, 0.043687816709280014, 0.010872420854866505, 0.008629326708614826, -0.06345818936824799, -0.024716762825846672, -0.01575188897550106, -0.03040086291730404, 0.011014130897819996, -0.026461465284228325, 0.015978137031197548, -0.0031363151501864195, -0.03639920428395271, 0.0026606384199112654, -0.03211615979671478, 0.03951142355799675, -0.0016638314118608832, 0.00725959800183773, -0.02500472404062748, 0.018065668642520905, 0.029186062514781952, -0.05522783100605011, 0.1066463440656662, -0.011534426361322403, 0.016731422394514084, 0.028347084298729897, -0.025037506595253944, 0.004856820218265057, 0.02435137890279293, -0.03807036951184273, 0.035146187990903854, 0.012982683256268501, 0.006744546815752983, 0.027694029733538628, 0.009473728947341442, 0.04641091451048851, -0.006112821400165558, 0.020845776423811913, -0.03937869891524315, 0.06729470193386078, 0.028710251674056053, 0.006596823688596487, -0.010389242321252823, -0.06836141645908356, -0.030509838834404945, 0.0003065116179641336, 0.038738250732421875, -0.2866021692752838, 0.03082023561000824, 0.022390808910131454, 0.027926046401262283, -0.04319196194410324, -0.02418563887476921, 0.04293077066540718, -0.04950174316763878, 0.03767028823494911, -0.050959862768650055, -0.030615858733654022, 0.040094200521707535, 0.045055877417325974, 0.050523389130830765, -0.058893680572509766, 0.02759312465786934, 0.06590566039085388, -0.021272484213113785, 0.04795074462890625, -0.014474483206868172, 0.008839116431772709, 0.016080427914857864, 0.18305373191833496, -0.06266050040721893, 0.044247690588235855, -0.01442289724946022, 0.0052406578324735165, -0.10249993205070496, 0.0537024587392807, -0.004709640052169561, -0.005211947485804558, -0.017864052206277847, 0.07110652327537537, -0.02195577695965767, 0.036020778119564056, 0.05523250997066498, 0.009995482861995697, 0.05972907319664955, -0.00757234962657094, -0.014578062109649181, -0.004501734860241413, -0.005102230701595545, 0.02950121834874153, -0.036854565143585205, 0.06819017231464386, 0.024107113480567932, 0.07595908641815186, -0.035726409405469894, -0.03779187798500061, -0.004570748191326857, -0.01373834814876318, -0.0019610829185694456, -0.04602067917585373, -0.029856782406568527, 0.03514508903026581, 0.039949506521224976, 0.004158783238381147, -0.013989849016070366, -0.010968433693051338, -0.028500499203801155, -0.019499020650982857, -0.027694175019860268, 0.035924818366765976, -0.045547015964984894, 0.010283676907420158], metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='126964cc-7055-4c6e-b79b-d9bda8825927', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='7f3f30aae12cc9e175355524bcbf078a75e18fdfc3afd81bc74a6574a21373a3')}, text='Methods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.\\nIn our approach, we use experts’ demonstrations to re-\\nshape the control problem as a search problem over a latent\\nspace of partial trajectories (called situations ). Our work as-\\nsumes that:\\n• Similar situations require similar solutions or actions.\\n• Asituation can be represented in a latent space.\\n* Equal contribution.\\nFigure 1: Similarity and divergence of ﬁve pairs of simu-\\nlator and dataset situation trajectories. Left column : cur-\\nrent frame from the MineRL environment. Middle col-\\numn : current reference frame from the dataset trajectory that\\nthe agent follows. Right column : L1-distance plot between\\nVPT embedding points of simulator and dataset situations .\\nThe current step shown as RGB images in the Left andMid-\\ndlecolumns is highlighted by the rightmost bold dot marker\\nin the Right column. X-axis indicates 256 time steps. Y-\\naxis indicates L1 distance between two embedding points\\nin the VPT latent space and ranges from 0.1 to 0.4. The de-\\nviation threshold is show by dashed red horizontal lines L1\\n= 0.35. Colored vertical lines mark new search events to ﬁnd\\nthe most similar situation in the dataset. Blue lines indicate\\ntime-based initiated searches, red lines indicate deviation-\\nthreshold based initiated searches. Gray dotted lines: every\\n64 steps in the X-axis and every 0.1 steps in the Y-axis.\\n• The situations latent space is a metric space. Therefore,\\nwe can assess the numerical similarity between any two\\nsituations .arXiv:2212.13326v2  [cs.LG]  17 Apr 2023', start_char_idx=1751, end_char_idx=3499, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex. Embedding some nodes\n",
    "embed_model(splitter.get_nodes_from_documents([documents[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Chroma VectorDB instance\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "chroma_collection = db.get_or_create_collection(\"rag\")\n",
    "\n",
    "# Create a Chroma VectorStore\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a Simple Document Store as well\n",
    "docstore = SimpleDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 86/86 [00:00<00:00, 117.19it/s]\n",
      "Generating embeddings: 100%|██████████| 307/307 [00:23<00:00, 12.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build the Embedding Pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        splitter,\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "_nodes = pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index for retrieval\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Explain behavioral cloning via search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='212b8134-5d22-42ab-93ca-4f4fc1634785', embedding=None, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='28219c85d590931b65fcf84700dc2228bb0e4f65a5cd935f58ec7b9775639446'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f7a45fbc-31b9-4936-aef6-626a484fecc0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dbc651dffb9d109abd81ea6a4de47b4281354da5c1bf3289da596491d5f2d125')}, text='Behavioral Cloning via Search in Video PreTraining Latent Space\\nFederico Malato*\\nUniversity of Eastern Finland\\nfmalato@uef.ﬁFlorian Leopold*\\nUniversity of Bielefeld\\nﬂeopold@techfak.uni-bielefeld.deAmogh Raut\\nIndian Institute of Technology BHU\\nVille Hautam ¨aki\\nUniversity of Eastern FinlandAndrew Melnik\\nUniversity of Bielefeld\\nAbstract\\nOur aim is to build autonomous agents that can solve\\ntasks in environments like Minecraft. To do so, we used\\nan imitation learning-based approach. We formulate our\\ncontrol problem as a search problem over a dataset of\\nexperts’ demonstrations, where the agent copies actions\\nfrom a similar demonstration trajectory of image-action\\npairs. We perform a proximity search over the BASALT\\nMineRL-dataset in the latent representation of a Video\\nPreTraining model. The agent copies the actions from\\nthe expert trajectory as long as the distance between the\\nstate representations of the agent and the selected ex-\\npert trajectory from the dataset do not diverge. Then\\nthe proximity search is repeated. Our approach can\\neffectively recover meaningful demonstration trajecto-\\nries and show human-like behavior of an agent in the\\nMinecraft environment.\\nIntroduction\\nThis study was motivated by the MineRL BASALT 2022\\nchallenge [1]. In the challenge, an agent must solve the fol-\\nlowing tasks: ﬁnd a cave, catch a pet, build a village house,\\nand make a waterfall [1]. The provided dataset of experts’\\ndemonstrations contains trajectories of image-action pairs.\\nAdditionally, both the MineRL BASALT dataset and envi-\\nronments do not contain reward information. Therefore, our\\nprimary focus was on Behavioural Cloning (BC) and Plan-\\nning [2][3] methods to address the tasks, rather than deep\\nreinforcement learning (DRL) [4][5].\\nMethods\\nA dataset of expert demonstrations solving the following\\ntasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\\nlage house, and make a waterfall. Each episode is a trajec-\\ntory of image-action pairs. No reward information is pro-\\nvided.', start_char_idx=0, end_char_idx=2002, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6896332236196575),\n",
       " NodeWithScore(node=TextNode(id_='913599c8-d6ce-4136-82a7-d6b7e8befb05', embedding=None, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/BC_via_Search_VPT.pdf_part_1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='2f45bb3ba35b60c48f3d03c6fc7a95082b4828ac42488fd8e13efaae93122fc9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6a26538c-0605-49dc-9ca7-4cf0a2aa473e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'BC_via_Search_VPT.pdf'}, hash='e2be83dbaf7bf61b1d0d2d0c833d2d8fa93fabb18bcbe019af108faa84876897'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2b973af7-a336-48ef-b616-646c223892c1', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c9eb3e919c5833773c77af78f15bfa93ae8f51f0e073fac91a6c81562cacb9f0')}, text='Additionally to the current frame, a memory stack\\nstores the last 128 embeddings for each transformer block.\\nThe output of the last transformer block are 129 embedding\\nvectors, each 1024-dimensional. The architecture discards\\n128 output embedding vectors of the last transformer block\\nand processes further only the current’s frame embedding\\nvector. Two MLP output heads take as an input the current’s\\nframe embedding vector to predict actions. The ﬁrst output\\nhead predicts a discrete action (one out of 8641 possible\\ncombinations of compound keyboard actions). The second\\noutput head predicts a computer mouse control as a discrete\\ncluster of one of the possible 121=11x11 mouse displace-\\nment regions (±5 regions for X times ±5 regions for Y). The\\narchitecture is shown in Figure 2.\\nSearch-based BC Search-based behavioral cloning (BC)\\naims to reproduce an expert’s behavior with high ﬁdelity by\\ncopying its solutions from past experience. We deﬁne a sit-\\nuation as a set {(oτ, aτ)}t+∆t\\nτ=tof consecutive observation-\\naction pairs coming from a set of provided expert’s trajecto-\\nries, where ∆tis less or equal to the number of input slots\\nof a transformer block that processes embedding vectors of\\ninput images.\\nWe encode the expert’s past situations through a provided\\nVPT model [6]. Thus, we obtain a latent space populated by\\nN-dimensional situation points. Due to the expert’s optimal-\\nity assumption, we can assume that each situation has been\\naddressed and solved in an optimal way.\\nVPT\\nDemonstration\\xa0\\nDatasetDataset Images\\nActionsVPT- Embeddings\\nSituations\\xa0\\nDataset\\nObserved Images VPT- EmbeddingsMost similar\\xa0\\nsituationL1 distance\\nMineRL\\xa0\\nEnvironmentSituations\\xa0\\nDataset\\nVPTActions\\nCop y\\xa0ActionsVPT- EmbeddingsA\\nBFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\\nation loop.\\nWe encode each sampled situation with the same net-\\nwork. Then, we search the nearest embedding point in the\\ndataset of situation points. Once the reference situation has\\nbeen selected, we copy its corresponding actions.', start_char_idx=1473, end_char_idx=3490, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6004311618081943),\n",
       " NodeWithScore(node=TextNode(id_='87133cd2-6817-4d1d-830b-19bd91984ebc', embedding=None, metadata={'page_label': '9', 'file_name': 'CraftAssist.pdf'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/mnt/d/Fidelity/LEAP Gen AI Workshop/pdf-chatbot-leap-workshop/documents/CraftAssist.pdf_part_8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'CraftAssist.pdf'}, hash='fc19cd51395451826d274505004c4ff736279247535777a5fc5e96600be06c13'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c53ff8f5-2363-4b48-9441-f8bc5f4a509b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'CraftAssist.pdf'}, hash='91cefbe675cd1dcf3c8cc6a5ae8bcbcbc6446cc555c567fd4b40edf951a16f82'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e858a790-e9c7-4206-beaf-1d5af8a4214c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='52a4b13f5e245de56637a857d33bd8ed898979bc95257084fd0e594685bbd48d')}, text='CraftAssist: A Framework for Dialogue-enabled Interactive Agents\\nare more ﬂexible formats, e.g. raw text; and there have\\nbeen recent successes using such memories, for example\\nworks using the Squad dataset (Rajpurkar et al., 2016). We\\nhope our platform will be useful for studying other sym-\\nbolic memory architectures as well as continuous, learned\\napproaches, and things in between.\\n8.3. Modularity for ML research\\nThe bot’s architecture is modular, and currently many of\\nthe modules are not learned. Many machine learning re-\\nsearchers consider the sort of tasks that pipelining makes\\nsimpler to be tools for evaluating more general learning\\nmethodologies. Such a researcher might advocate more\\nend-to-end (or otherwise less “engineered”) approaches be-\\ncause the goal is not necessarily to build something that\\nworks well on the tasks that the engineered approach can\\nsucceed in, but rather to build something that can scale be-\\nyond those tasks.\\nWe have chosen this approach in part because it allows us to\\nmore easily build an interesting initial assistant from which\\nwe can iterate; and in particular allows data collection and\\ncreation. We do believe that modular systems are more\\ngenerally interesting, especially in the setting of compe-\\ntency across a large number of relatively easier tasks. Per-\\nhaps most interesting to us are approaches that allow mod-\\nular components with clearly deﬁned interfaces, and het-\\nerogeneous training based on what data is available. We\\nhope to explore these with further iterations of our bot.\\nDespite our own pipelined approach, we consider research\\non more end-to-end approaches worthwhile and interest-\\ning. Even for researchers primarily interested in these, the\\npipelined approach still has value beyond serving as a base-\\nline: as discussed above, it allows generating large amounts\\nof training data for end-to-end methods.\\nFinally, we note that from an engineering standpoint, mod-\\nularity has clear beneﬁts. In particular, it allows many re-\\nsearchers to contribute components to the greater whole\\nin parallel. As discussed above, the bot presented here is\\nmeant to be a jumping off point, not a ﬁnal product.', start_char_idx=0, end_char_idx=2162, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5843233513776804)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Behavioral Cloning via Search in Video PreTraining Latent Space\n",
      "Federico Malato*\n",
      "University of Eastern Finland\n",
      "fmalato@uef.ﬁFlorian Leopold*\n",
      "University of Bielefeld\n",
      "ﬂeopold@techfak.uni-bielefeld.deAmogh Raut\n",
      "Indian Institute of Technology BHU\n",
      "Ville Hautam ¨aki\n",
      "University of Eastern FinlandAndrew Melnik\n",
      "University of Bielefeld\n",
      "Abstract\n",
      "Our aim is to build autonomous agents that can solve\n",
      "tasks in environments like Minecraft. To do so, we used\n",
      "an imitation learning-based approach. We formulate our\n",
      "control problem as a search problem over a dataset of\n",
      "experts’ demonstrations, where the agent copies actions\n",
      "from a similar demonstration trajectory of image-action\n",
      "pairs. We perform a proximity search over the BASALT\n",
      "MineRL-dataset in the latent representation of a Video\n",
      "PreTraining model. The agent copies the actions from\n",
      "the expert trajectory as long as the distance between the\n",
      "state representations of the agent and the selected ex-\n",
      "pert trajectory from the dataset do not diverge. Then\n",
      "the proximity search is repeated. Our approach can\n",
      "effectively recover meaningful demonstration trajecto-\n",
      "ries and show human-like behavior of an agent in the\n",
      "Minecraft environment.\n",
      "Introduction\n",
      "This study was motivated by the MineRL BASALT 2022\n",
      "challenge [1]. In the challenge, an agent must solve the fol-\n",
      "lowing tasks: ﬁnd a cave, catch a pet, build a village house,\n",
      "and make a waterfall [1]. The provided dataset of experts’\n",
      "demonstrations contains trajectories of image-action pairs.\n",
      "Additionally, both the MineRL BASALT dataset and envi-\n",
      "ronments do not contain reward information. Therefore, our\n",
      "primary focus was on Behavioural Cloning (BC) and Plan-\n",
      "ning [2][3] methods to address the tasks, rather than deep\n",
      "reinforcement learning (DRL) [4][5].\n",
      "Methods\n",
      "A dataset of expert demonstrations solving the following\n",
      "tasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\n",
      "lage house, and make a waterfall. Each episode is a trajec-\n",
      "tory of image-action pairs. No reward information is pro-\n",
      "vided.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_nodes[0].node.get_content(metadata_mode='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"YOUR API KEY HERE\"\n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_prompt(retrieved_nodes, query_str):\n",
    "    context_str = \"\\n--------------------\\n\".join([node.get_content(metadata_mode='all') for node in retrieved_nodes])\n",
    "    prompt = f\"Given the following context: \\n{context_str}\\n--------------------\\n\\nPlease answer the following query given the above context: \\n{query_str}\\n\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following context: \n",
      "page_label: 1\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Behavioral Cloning via Search in Video PreTraining Latent Space\n",
      "Federico Malato*\n",
      "University of Eastern Finland\n",
      "fmalato@uef.ﬁFlorian Leopold*\n",
      "University of Bielefeld\n",
      "ﬂeopold@techfak.uni-bielefeld.deAmogh Raut\n",
      "Indian Institute of Technology BHU\n",
      "Ville Hautam ¨aki\n",
      "University of Eastern FinlandAndrew Melnik\n",
      "University of Bielefeld\n",
      "Abstract\n",
      "Our aim is to build autonomous agents that can solve\n",
      "tasks in environments like Minecraft. To do so, we used\n",
      "an imitation learning-based approach. We formulate our\n",
      "control problem as a search problem over a dataset of\n",
      "experts’ demonstrations, where the agent copies actions\n",
      "from a similar demonstration trajectory of image-action\n",
      "pairs. We perform a proximity search over the BASALT\n",
      "MineRL-dataset in the latent representation of a Video\n",
      "PreTraining model. The agent copies the actions from\n",
      "the expert trajectory as long as the distance between the\n",
      "state representations of the agent and the selected ex-\n",
      "pert trajectory from the dataset do not diverge. Then\n",
      "the proximity search is repeated. Our approach can\n",
      "effectively recover meaningful demonstration trajecto-\n",
      "ries and show human-like behavior of an agent in the\n",
      "Minecraft environment.\n",
      "Introduction\n",
      "This study was motivated by the MineRL BASALT 2022\n",
      "challenge [1]. In the challenge, an agent must solve the fol-\n",
      "lowing tasks: ﬁnd a cave, catch a pet, build a village house,\n",
      "and make a waterfall [1]. The provided dataset of experts’\n",
      "demonstrations contains trajectories of image-action pairs.\n",
      "Additionally, both the MineRL BASALT dataset and envi-\n",
      "ronments do not contain reward information. Therefore, our\n",
      "primary focus was on Behavioural Cloning (BC) and Plan-\n",
      "ning [2][3] methods to address the tasks, rather than deep\n",
      "reinforcement learning (DRL) [4][5].\n",
      "Methods\n",
      "A dataset of expert demonstrations solving the following\n",
      "tasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\n",
      "lage house, and make a waterfall. Each episode is a trajec-\n",
      "tory of image-action pairs. No reward information is pro-\n",
      "vided.\n",
      "--------------------\n",
      "page_label: 2\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Additionally to the current frame, a memory stack\n",
      "stores the last 128 embeddings for each transformer block.\n",
      "The output of the last transformer block are 129 embedding\n",
      "vectors, each 1024-dimensional. The architecture discards\n",
      "128 output embedding vectors of the last transformer block\n",
      "and processes further only the current’s frame embedding\n",
      "vector. Two MLP output heads take as an input the current’s\n",
      "frame embedding vector to predict actions. The ﬁrst output\n",
      "head predicts a discrete action (one out of 8641 possible\n",
      "combinations of compound keyboard actions). The second\n",
      "output head predicts a computer mouse control as a discrete\n",
      "cluster of one of the possible 121=11x11 mouse displace-\n",
      "ment regions (±5 regions for X times ±5 regions for Y). The\n",
      "architecture is shown in Figure 2.\n",
      "Search-based BC Search-based behavioral cloning (BC)\n",
      "aims to reproduce an expert’s behavior with high ﬁdelity by\n",
      "copying its solutions from past experience. We deﬁne a sit-\n",
      "uation as a set {(oτ, aτ)}t+∆t\n",
      "τ=tof consecutive observation-\n",
      "action pairs coming from a set of provided expert’s trajecto-\n",
      "ries, where ∆tis less or equal to the number of input slots\n",
      "of a transformer block that processes embedding vectors of\n",
      "input images.\n",
      "We encode the expert’s past situations through a provided\n",
      "VPT model [6]. Thus, we obtain a latent space populated by\n",
      "N-dimensional situation points. Due to the expert’s optimal-\n",
      "ity assumption, we can assume that each situation has been\n",
      "addressed and solved in an optimal way.\n",
      "VPT\n",
      "Demonstration \n",
      "DatasetDataset Images\n",
      "ActionsVPT- Embeddings\n",
      "Situations \n",
      "Dataset\n",
      "Observed Images VPT- EmbeddingsMost similar \n",
      "situationL1 distance\n",
      "MineRL \n",
      "EnvironmentSituations \n",
      "Dataset\n",
      "VPTActions\n",
      "Cop y ActionsVPT- EmbeddingsA\n",
      "BFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\n",
      "ation loop.\n",
      "We encode each sampled situation with the same net-\n",
      "work. Then, we search the nearest embedding point in the\n",
      "dataset of situation points. Once the reference situation has\n",
      "been selected, we copy its corresponding actions.\n",
      "--------------------\n",
      "page_label: 9\n",
      "file_name: CraftAssist.pdf\n",
      "\n",
      "CraftAssist: A Framework for Dialogue-enabled Interactive Agents\n",
      "are more ﬂexible formats, e.g. raw text; and there have\n",
      "been recent successes using such memories, for example\n",
      "works using the Squad dataset (Rajpurkar et al., 2016). We\n",
      "hope our platform will be useful for studying other sym-\n",
      "bolic memory architectures as well as continuous, learned\n",
      "approaches, and things in between.\n",
      "8.3. Modularity for ML research\n",
      "The bot’s architecture is modular, and currently many of\n",
      "the modules are not learned. Many machine learning re-\n",
      "searchers consider the sort of tasks that pipelining makes\n",
      "simpler to be tools for evaluating more general learning\n",
      "methodologies. Such a researcher might advocate more\n",
      "end-to-end (or otherwise less “engineered”) approaches be-\n",
      "cause the goal is not necessarily to build something that\n",
      "works well on the tasks that the engineered approach can\n",
      "succeed in, but rather to build something that can scale be-\n",
      "yond those tasks.\n",
      "We have chosen this approach in part because it allows us to\n",
      "more easily build an interesting initial assistant from which\n",
      "we can iterate; and in particular allows data collection and\n",
      "creation. We do believe that modular systems are more\n",
      "generally interesting, especially in the setting of compe-\n",
      "tency across a large number of relatively easier tasks. Per-\n",
      "haps most interesting to us are approaches that allow mod-\n",
      "ular components with clearly deﬁned interfaces, and het-\n",
      "erogeneous training based on what data is available. We\n",
      "hope to explore these with further iterations of our bot.\n",
      "Despite our own pipelined approach, we consider research\n",
      "on more end-to-end approaches worthwhile and interest-\n",
      "ing. Even for researchers primarily interested in these, the\n",
      "pipelined approach still has value beyond serving as a base-\n",
      "line: as discussed above, it allows generating large amounts\n",
      "of training data for end-to-end methods.\n",
      "Finally, we note that from an engineering standpoint, mod-\n",
      "ularity has clear beneﬁts. In particular, it allows many re-\n",
      "searchers to contribute components to the greater whole\n",
      "in parallel. As discussed above, the bot presented here is\n",
      "meant to be a jumping off point, not a ﬁnal product.\n",
      "--------------------\n",
      "\n",
      "Please answer the following query given the above context: \n",
      "Explain behavioral cloning via search\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a context prompt string based on the retrieved nodes and the query\n",
    "print(build_context_prompt(retrieved_nodes, query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(build_context_prompt(retrieved_nodes, query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Behavioral Cloning (BC) via Search is a method used to build autonomous agents that can solve tasks in environments like Minecraft. Here's a breakdown of the approach:\n",
      "\n",
      "**Overview**\n",
      "\n",
      "The goal is to build an agent that can mimic human-like behavior in a Minecraft environment by copying actions from expert demonstrations.\n",
      "\n",
      "**Key Components**\n",
      "\n",
      "1. **Expert Demonstrations**: A dataset of expert trajectories is provided, containing image-action pairs.\n",
      "2. **Video PreTraining (VPT) Model**: A pre-trained model is used to encode the expert demonstrations into a latent space.\n",
      "3. **Search**: The agent performs a proximity search in the latent space to find the most similar situation (i.e., a set of consecutive observation-action pairs) to the current state of the agent.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "1. The agent encodes the current situation using the VPT model.\n",
      "2. The agent searches for the nearest embedding point in the dataset of situation points.\n",
      "3. Once the reference situation is selected, the agent copies the corresponding actions from the expert trajectory.\n",
      "\n",
      "**Rationale**\n",
      "\n",
      "The assumption is that each situation in the expert demonstrations has been addressed and solved in an optimal way. By searching for the most similar situation in the latent space, the agent can effectively recover meaningful demonstration trajectories and exhibit human-like behavior in the Minecraft environment.\n",
      "\n",
      "In summary, Behavioral Cloning via Search is a method that leverages expert demonstrations, a pre-trained VPT model, and proximity search to enable an agent to mimic human-like behavior in a Minecraft environment.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot \n",
    "<h3>For Conversational Question Answering over PDF Documents</h3>\n",
    "\n",
    "Now that we have a basic RAG pipeline in place, we can extend it to a conversational setting. We can use the same retrieval and generation components as before, but we also need to keep track of the conversation but more importantly, we need to intelligently call the RAG pipeline based on the context. We will create an Agent with function calling capabilities to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.agent.openai import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: If you have an OpenAI key, you can run this cell, else skip it.\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# OPENAI_API_KEY = \"YOUR API KEY HERE\"\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Agent Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "\n",
    "\n",
    "def useless_tool() -> int:\n",
    "    \"\"\"This is a uselss tool.\"\"\"\n",
    "    return \"This is a uselss output.\"\n",
    "\n",
    "\n",
    "useless_tool = FunctionTool.from_defaults(fn=useless_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a system message for tool usage\n",
    "system_message = ChatMessage(\n",
    "    role='system',\n",
    "    content=\"\"\"You are a simple chatbot that has access to some tools.\n",
    "You have access to the `add` function which can be used to add up any numbers in the query.\n",
    "You don't need to call this function always, but you can use it when you need to do some addition-like calculation based on the query.\n",
    "Use only when absolutely necessary.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the OpenAIAgent class to use function calling capabilities with our Groq LLM\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[useless_tool, add_tool], \n",
    "    llm=llm, \n",
    "    chat_history=[system_message],\n",
    "    temperature=0,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: help me settle a bet between me and my friend. I say 20 + 50 is equal to 70 but my friend says 80. who is correct?\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":20,\"b\":50}\n",
      "Got output: 70\n",
      "========================\n",
      "\n",
      "You are correct! 20 + 50 equals 70.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"help me settle a bet between me and my friend. I say 20 + 50 is equal to 70 but my friend says 80. who is correct?\", tool_choice=\"auto\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: yay! I won\n",
      "Congratulations on winning the bet! If you have any more questions or need assistance with anything else, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"yay! I won\", tool_choice=\"auto\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_fn(query: str) -> str:\n",
    "    \"\"\"This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string containing the retrieved context chunks from the document.\n",
    "    \"\"\"\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    context_str = \"\\n--------------------\\n\".join([node.get_content(metadata_mode='all') for node in retrieved_nodes])\n",
    "    return \"Contextual information to answer the query is below:\\n\" + context_str\n",
    "\n",
    "retrieval_tool = FunctionTool.from_defaults(fn=retrieval_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = ChatMessage(\n",
    "    role='system',\n",
    "    content=\"\"\"You are a Q&A chatbot designed to answer questions based on a set of AI research paper PDFs. \n",
    "Your responses should be solely based on the information retrieved from these documents. \n",
    "\n",
    "To answer a question, first check if the answer can be derived from the current chat history context. If it can't,\n",
    "you need to call the retrieval_fn() function with the appropriate search query. \n",
    "\n",
    "The retrieval_fn() function should only be called once per turn, ensuring efficient use of resources. \n",
    "If the function doesn't return the necessary information to support your response, you should inform the user that \n",
    "you are unable to provide an answer.\n",
    "\n",
    "Remember, every answer you provide must be backed by a document retrieved from the retrieval_fn() function. This \n",
    "ensures that all responses are based on credible and verifiable information.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = OpenAIAgent.from_tools(\n",
    "    tools=[retrieval_tool],\n",
    "    llm=llm,\n",
    "    chat_history=[system_message],\n",
    "    temperature=0.1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are a Q&A chatbot designed to answer questions based on a set of AI research paper PDFs. \\nYour responses should be solely based on the information retrieved from these documents. \\n\\nTo answer a question, first check if the answer can be derived from the current chat history context. If it can't,\\nyou need to call the retrieval_fn() function with the appropriate search query. \\n\\nThe retrieval_fn() function should only be called once per turn, ensuring efficient use of resources. \\nIf the function doesn't return the necessary information to support your response, you should inform the user that \\nyou are unable to provide an answer.\\n\\nRemember, every answer you provide must be backed by a document retrieved from the retrieval_fn() function. This \\nensures that all responses are based on credible and verifiable information.\\n\", additional_kwargs={})]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain behavioral cloning via search\n",
      "=== Calling Function ===\n",
      "Calling function: retrieval_fn with args: {\"query\":\"behavioral cloning\"}\n",
      "Got output: Contextual information to answer the query is below:\n",
      "page_label: 1\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Behavioral Cloning via Search in Video PreTraining Latent Space\n",
      "Federico Malato*\n",
      "University of Eastern Finland\n",
      "fmalato@uef.ﬁFlorian Leopold*\n",
      "University of Bielefeld\n",
      "ﬂeopold@techfak.uni-bielefeld.deAmogh Raut\n",
      "Indian Institute of Technology BHU\n",
      "Ville Hautam ¨aki\n",
      "University of Eastern FinlandAndrew Melnik\n",
      "University of Bielefeld\n",
      "Abstract\n",
      "Our aim is to build autonomous agents that can solve\n",
      "tasks in environments like Minecraft. To do so, we used\n",
      "an imitation learning-based approach. We formulate our\n",
      "control problem as a search problem over a dataset of\n",
      "experts’ demonstrations, where the agent copies actions\n",
      "from a similar demonstration trajectory of image-action\n",
      "pairs. We perform a proximity search over the BASALT\n",
      "MineRL-dataset in the latent representation of a Video\n",
      "PreTraining model. The agent copies the actions from\n",
      "the expert trajectory as long as the distance between the\n",
      "state representations of the agent and the selected ex-\n",
      "pert trajectory from the dataset do not diverge. Then\n",
      "the proximity search is repeated. Our approach can\n",
      "effectively recover meaningful demonstration trajecto-\n",
      "ries and show human-like behavior of an agent in the\n",
      "Minecraft environment.\n",
      "Introduction\n",
      "This study was motivated by the MineRL BASALT 2022\n",
      "challenge [1]. In the challenge, an agent must solve the fol-\n",
      "lowing tasks: ﬁnd a cave, catch a pet, build a village house,\n",
      "and make a waterfall [1]. The provided dataset of experts’\n",
      "demonstrations contains trajectories of image-action pairs.\n",
      "Additionally, both the MineRL BASALT dataset and envi-\n",
      "ronments do not contain reward information. Therefore, our\n",
      "primary focus was on Behavioural Cloning (BC) and Plan-\n",
      "ning [2][3] methods to address the tasks, rather than deep\n",
      "reinforcement learning (DRL) [4][5].\n",
      "Methods\n",
      "A dataset of expert demonstrations solving the following\n",
      "tasks was provided [1]: ﬁnd a cave, catch a pet, build a vil-\n",
      "lage house, and make a waterfall. Each episode is a trajec-\n",
      "tory of image-action pairs. No reward information is pro-\n",
      "vided.\n",
      "--------------------\n",
      "page_label: 2\n",
      "file_name: BC_via_Search_VPT.pdf\n",
      "\n",
      "Additionally to the current frame, a memory stack\n",
      "stores the last 128 embeddings for each transformer block.\n",
      "The output of the last transformer block are 129 embedding\n",
      "vectors, each 1024-dimensional. The architecture discards\n",
      "128 output embedding vectors of the last transformer block\n",
      "and processes further only the current’s frame embedding\n",
      "vector. Two MLP output heads take as an input the current’s\n",
      "frame embedding vector to predict actions. The ﬁrst output\n",
      "head predicts a discrete action (one out of 8641 possible\n",
      "combinations of compound keyboard actions). The second\n",
      "output head predicts a computer mouse control as a discrete\n",
      "cluster of one of the possible 121=11x11 mouse displace-\n",
      "ment regions (±5 regions for X times ±5 regions for Y). The\n",
      "architecture is shown in Figure 2.\n",
      "Search-based BC Search-based behavioral cloning (BC)\n",
      "aims to reproduce an expert’s behavior with high ﬁdelity by\n",
      "copying its solutions from past experience. We deﬁne a sit-\n",
      "uation as a set {(oτ, aτ)}t+∆t\n",
      "τ=tof consecutive observation-\n",
      "action pairs coming from a set of provided expert’s trajecto-\n",
      "ries, where ∆tis less or equal to the number of input slots\n",
      "of a transformer block that processes embedding vectors of\n",
      "input images.\n",
      "We encode the expert’s past situations through a provided\n",
      "VPT model [6]. Thus, we obtain a latent space populated by\n",
      "N-dimensional situation points. Due to the expert’s optimal-\n",
      "ity assumption, we can assume that each situation has been\n",
      "addressed and solved in an optimal way.\n",
      "VPT\n",
      "Demonstration \n",
      "DatasetDataset Images\n",
      "ActionsVPT- Embeddings\n",
      "Situations \n",
      "Dataset\n",
      "Observed Images VPT- EmbeddingsMost similar \n",
      "situationL1 distance\n",
      "MineRL \n",
      "EnvironmentSituations \n",
      "Dataset\n",
      "VPTActions\n",
      "Cop y ActionsVPT- EmbeddingsA\n",
      "BFigure 3: Our approach. ( A) Training procedure. ( B) Evalu-\n",
      "ation loop.\n",
      "We encode each sampled situation with the same net-\n",
      "work. Then, we search the nearest embedding point in the\n",
      "dataset of situation points. Once the reference situation has\n",
      "been selected, we copy its corresponding actions.\n",
      "--------------------\n",
      "page_label: 14\n",
      "file_name: Mamba.pdf\n",
      "\n",
      "0000000d\n",
      "/uni00000037/uni00000018/uni0000000f/uni0000002a/uni0000002a/uni00000032\n",
      "/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045Figure 7: ( Audio Pretraining .) Mamba improves performance\n",
      "over prior state-of-the-art (Sashimi) in autoregressive audio mod-\n",
      "eling, while improving up to minute-long context or million-\n",
      "length sequences (controlling for computation).\n",
      "which may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not\n",
      "control for computation time.\n",
      "4.3.3 Synthetic Species Classi/f_ication\n",
      "We evaluate models on a downstream task of classifying between 5 diﬀerent species by randomly sampling a contigu-\n",
      "ous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human ,lemur ,mouse ,pig,hippo }.\n",
      "We modify the task to be signiﬁcantly more challenging by classifying between the ﬁve great apes species\n",
      "{human ,chimpanzee ,gorilla ,orangutan ,bonobo }, which are known to share 99% of their DNA.\n",
      "4.4 Audio Modeling and Generation\n",
      "For the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel\n",
      "et al. 2022). This model comprises\n",
      "1.a U-Net backbone with two stages of pooling by a factor pthat doubles the model dimension /u1D437per stage,\n",
      "2.alternating S4 and MLP blocks in each stage.\n",
      "We consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.\n",
      "========================\n",
      "\n",
      "Behavioral cloning via search involves reproducing an expert's behavior with high fidelity by copying its solutions from past experience. In this approach, a situation is defined as a set of consecutive observation-action pairs from expert trajectories. The expert's past situations are encoded through a Video PreTraining (VPT) model to obtain a latent space populated by N-dimensional situation points. Each situation is assumed to have been addressed and solved optimally by the expert. The search-based behavioral cloning then involves encoding each sampled situation with the same network, searching for the nearest embedding point in the dataset of situation points, and copying the corresponding actions from the selected reference situation. This method aims to recover meaningful demonstration trajectories and demonstrate human-like behavior of an agent in environments like Minecraft.\n"
     ]
    }
   ],
   "source": [
    "response = rag_agent.chat(\n",
    "    \"Explain behavioral cloning via search\", tool_choice=\"auto\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: ELI5 your answer\n",
      "Behavioral cloning via search is like teaching a computer to imitate an expert by looking at how the expert solves problems. The computer learns by studying examples of the expert's actions and then tries to mimic those actions in similar situations. It uses a special model to understand and remember how the expert behaves optimally. By searching for similar situations in a dataset and copying the expert's actions, the computer can learn to act like the expert and perform tasks in a human-like way.\n"
     ]
    }
   ],
   "source": [
    "response = rag_agent.chat(\n",
    "    \"ELI5 your answer\"\n",
    ")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
